# **Cerina Protocol Foundry â€” Human-in-the-Loop AI Agent Workflow (LangGraph + FastAPI + React)**

This project implements a **multi-agent clinical content generation system** powered by **LangGraph**, **FastAPI**, **SQLite Checkpointing**, and a **React dashboard** for human-in-the-loop interaction.

It replicates the essential logic of **Cerinaâ€™s clinician-aligned workflows**:
draft â†’ safety check â†’ critique â†’ refinement â†’ supervisor pause â†’ human approval â†’ finalization â†’ summary.

The system runs **fully locally** using:

* **Mock LLM mode**
* Optionally: **Ollama**, **Local Transformers**, or any LangChain-compatible LLM

---

---

# â­ **Features**

### âœ… **1. Multi-Agent LangGraph Workflow**

The orchestrator automatically runs:

| Agent              | Role                                              |
| ------------------ | ------------------------------------------------- |
| **Draftsman**      | Generates the first draft of the CBT exercise.    |
| **SafetyGuardian** | Flags unsafe / harmful / unethical content.       |
| **ClinicalCritic** | Improves empathy, clarity, clinical quality.      |
| **Supervisor**     | Decides pause/resume; hands off to human.         |
| **Human**          | Reviewer/editor inside the UI.                    |
| **SummaryAgent**   | Produces a final 3â€“5 line human-friendly summary. |

---

### âœ… **2. Human-In-The-Loop Dashboard (React)**

The UI allows:

* âœ¨ Start a run
* ğŸ“ Provide or edit **intent text**
* ğŸ§  Watch agent events in **live event stream (SSE)**
* âœï¸ Edit the draft manually
* âœ”ï¸ Approve & finalize
* ğŸ“„ Generate summary
* ğŸ“¥ Export run data as JSON
* ğŸ“Œ Intent updating mid-run (PATCH)

---

### âœ… **3. Checkpointing + Replay**

Every agent writes a checkpoint snapshot:

```
{
  "agent": "ClinicalCritic",
  "timestamp": "...",
  "note": "Critic completed",
  "state": { ... }
}
```

Stored in SQLite:
`backend/app/checkpointer.py`.

You can:

* `/status/{run_id}` â†’ get latest state
* `/history/{run_id}` â†’ get entire timeline
* `/stream/{run_id}` â†’ SSE real-time events

---

### âœ… **4. Mock LLM / Local LLM support**

The system works **out of the box** with:

* Mock LLM mode
* Ollama (`llama3`, `mistral`, `phi3`, etc.)
* HuggingFace Transformers local models

Configured in:
`backend/app/llm_client.py`.

---

---

# ğŸš€ **1. Installation**

## **Clone the repo**

```bash
git clone https://github.com/<your-repo>/cerina-protocol-foundry
cd cerina-protocol-foundry
```

---

# ğŸš€ **2. Backend Setup (FastAPI + LangGraph)**

### **Create virtual environment**

```bash
python -m venv .venv
source .venv/Scripts/activate  # Windows
```

### **Install requirements**

```bash
pip install -r backend/requirements.txt
```

If missing:

```bash
pip install langgraph fastapi uvicorn sqlmodel pydantic transformers
```

### **Run backend**

```bash
./.venv/Scripts/python -m uvicorn backend.app.main:app --reload --port 8000 --host 127.0.0.1
```

You should see:

```
Cerina Foundry backend is running
```

Open API docs:
ğŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

---

# ğŸš€ **3. Frontend Setup (React)**

```bash
cd frontend/web
npm install --legacy-peer-deps
npm start
```

UI runs at:
ğŸ‘‰ [http://localhost:3000](http://localhost:3000)

---

---

# ğŸ“ **Project Structure**

```
cerina-protocol-foundry/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                    # FastAPI server
â”‚   â”‚   â”œâ”€â”€ orchestrator_langgraph_true.py
â”‚   â”‚   â”œâ”€â”€ checkpointer.py            # SQLite checkpointing
â”‚   â”‚   â”œâ”€â”€ llm_client.py              # Mock/LLM integration
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”‚   â”œâ”€â”€ draftsman.py
â”‚   â”‚   â”‚   â”œâ”€â”€ safety_guardian.py
â”‚   â”‚   â”‚   â”œâ”€â”€ critic.py
â”‚   â”‚   â”‚   â””â”€â”€ supervisor.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ web/
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â””â”€â”€ App.tsx                # Full dashboard UI
â”‚       â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md
```

---

---

# âš™ï¸ **4. API Overview**

### **Start a run**

```bash
POST /start
{
  "intent": "Create a CBT exercise for insomnia"
}
```

### **Stream live events (SSE)**

```
GET /stream/{run_id}
```

### **Check run status**

```
GET /status/{run_id}
```

### **Get full history**

```
GET /history/{run_id}
```

### **Approve & finalize**

```bash
POST /approve/{run_id}
{
  "text": "Looks good â€” finalize this version."
}
```

### **Update intent mid-run**

```bash
PATCH /run/{run_id}/intent
{
  "intent": "Rewrite for clarity and add a breathing step"
}
```

### **Generate summary**

```
GET /summary/{run_id}
```

---

---

# ğŸ§  **5. How the System Works (Detailed Explanation)**

When a user **starts a run**, the following happens:

---

## **Step 1 â€” Draftsman Agent**

* Reads the **intent text**
* Generates a **first draft**
* Example: *CBT exercise for insomnia focusing on stimulus control*

---

## **Step 2 â€” SafetyGuardian**

* Analyzes draft:

  * self-harm content
  * medical misinformation
  * unethical recommendations

If unsafe â†’ flags + rewrites

---

## **Step 3 â€” ClinicalCritic**

* Improves:

  * empathy
  * clarity
  * readability
  * clinical relevance

---

## **Step 4 â€” Supervisor**

* Pauses for human review
* Final editable draft sent to UI
* State: `"paused_for_human"`

---

## **Step 5 â€” Human Edits (UI)**

User may:

* edit the draft
* modify intent
* request new summary
* approve final version

---

## **Step 6 â€” Finalization**

After approval:

* system produces the **final state**
* SummaryAgent creates a 3â€“5 line summary
* Stored in `final_summary`

---

---

# ğŸ“ **6. Example Summary Output**

Example generated summary:

> A structured CBT exercise introducing nighttime breathing practice, cognitive reframing, and sleep-onset reduction strategies.
> Tailored for individuals with insomnia and includes a brief rationale for each step.
> Suitable for clinical review and user-facing instruction.

---

---

# ğŸ› ï¸ **7. Troubleshooting**

### **UI shows truncated summary ("â€¦")**

This is **UI text overflow**, not backend.
Use the *copy* button to confirm full text exists.

Fix included in latest UI version â€” long summaries now expand vertically.

---

### **ModuleNotFoundError: sqlmodel**

Install again inside venv:

```bash
./.venv/Scripts/pip install sqlmodel
```

---

### **React dependency conflicts**

Use:

```bash
npm install --legacy-peer-deps
```

---

### **SSE stream not updating**

Check CORS rules in `main.py`.

---

---

# ğŸ”§ **8. Extending the System**

You can easily add:

### â• **New Agents**

Drop a new file under `backend/app/agents/` and add it to the LangGraph workflow.

### ğŸ¤ Voice input

Integrate Whisper or VAD in frontend.

### ğŸ“„ Export PDF

Use Pythonâ€™s reportlab.

### ğŸ” Authentication

Add JWT middleware to FastAPI.

---

---

# ğŸ‰ **9. Credits**

Built for educational + prototype purposes using:

* **LangGraph**
* **FastAPI**
* **React (Vite)**
* **SQLite Checkpoints**
* **Ollama / Transformers**

---

Here is a **clean, professional architecture diagram** of your Cerina Protocol Foundry system, showing **LangGraph agents**, **FastAPI backend**, **checkpointing**, **frontend UI**, and **LLM integration**.

---